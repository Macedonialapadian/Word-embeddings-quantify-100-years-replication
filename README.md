## Overview

This repository contains a replication project for the **Text as Data** course, conducted by **Group: Zhiyang Cheng and Muhammad Saad**.

This project aims at replicating Garg, Nikhil, et al. "Word embeddings quantify 100 years of gender and ethnic stereotypes." Proceedings of the National Academy of Sciences 115.16 (2018): E3635-E3644. which can be find through this [link](https://www.pnas.org/doi/pdf/10.1073/pnas.1720347115)

## Directory Setup

- **Project root:** `EmbeddingDynamicStereotypes-master/`  
  All scripts should be run from this directory.

- **Vectors folder:** `vector/`  
  The `vector` folder must be placed directly under the project root. It should contain the required word vector files (download separately from this [link](https://drive.google.com/drive/folders/1p9-WFXGy-cPFKRNkGl3pi71caAsOhe3x?usp=drive_link)).

- **Replication scripts:** `replication_scripts/`  
  This folder contains our modified versions of the author’s scripts used for replication as well as other scripts taht helped us with the replication process

- **Results folder:** `run_rusults/`  
  This folder stores the results generated by `changes_over_time.py`.  
 
- **Plot folder:** `Plot/`  
  This folder stores the results generated by `create_final_plots_all.py`. the 'used in essay' folder contains the plots that we selected that appears in the essay

- **Figure 7:** `Figure 7/` 
  THE 'Data' folder under 'Figure 7' is empty, it should be placed with 'newsroom_test.jsonl' files download from this [link](https://drive.google.com/drive/folders/1OpWGtNB-IJTg4nE6U8yRb41mKaPVk-oP?usp=drive_link), and it is originally come from this [project](https://lil.nlp.cornell.edu/newsroom/index.html)

## Replication Steps

The author’s repository is **`EmbeddingDynamicStereotypes-master`**, which we refer to as the *project root*. All scripts are taking this as working directory.

To replicate the results:

1. Ensure the `vector` folder is placed under the project root (`EmbeddingDynamicStereotypes-master/vector/`).Both 'raw' and normalized_clean must all exists
  -raw folder contains raw materials, while normalized_clean folder contains cleaned material.
  -changes_over_time.py will only use materials in normalized_clean.

Now we do data preprocessing,the script mainly use the data in the raw folder to generate organized data in normalized_clean
2. From the project root, run `orgnize_COHA.py`.
  -this script create word counts(within vocab folder) and vectors in the normalized_clean folder from word counts, vocabulary and word vectors in raw folder.
  -the COHA word vectors are themselves normalized, so there is no need to normalize them.
3. Run three normalizer scripts (`normalizer_glove.py`,`normalizer_googlenews.py`,`normalizer_wikipedia.py`).
  -this script convert three word vectors to txt and normalize them.
 
Then we can start to calculate the results and generate the plots
4. Run `changes_over_time.py`.
  -This create finalrun.csv in run_results. It will add content at the end of the csv, so remove the original finalrun.csv each time you run this script
  -This script uses run_params.csv, files in normalized_clean, and word lists in the data folder
  -Note: Even though it's written in the script, `run_rusults` is not part of the original repository; it is created during replication.
5. Run `create_final_plots_all.py`.
  -It only use finalrun.csv
  -This creates the plot in plot folder and regression results in regressions folder(which is created by plot_creation.py)
  -This also uses `latexify.py`, `plot_creation.py` and `utilities.py`

## modification to the author's scripts (except changes from python2 to python3)
1. `changes_over_time.py`
  - comment nyt(new york time data) related in line 271,279,287
  
2. `latexify.py`
  - change the parameter "text.usetex" to False. this is no longer support in the latest version of package, and it was used to create confidential intervals.

3. `utilities.py`
  - there are a few little changes and i think the results will be the same.

4. `plot_creation.py`
  - line 798
  - line 834
  - line 859
  - line 113
  - line 118
  - line 413

5. `create_final_plots_all.py`
  - comment nyt related in line 65
  
there are many little modifications, but I think only the small changes in `utilities.py` and `plot_creation.py` might be concerning if you might want to double check.

6. The replace plot for NYT data
  - in the 'Figure 7'folder, run the jupyter notebook under 'Notebook' to get the figure 7 that use Newsroom_Embeddings as replacement for NYT embeddings

## use_less_scripts
use_less_scripts folder is under the replication_scripts folder, which contains the scripts that we once used for analysis. they should also be run taking the project root as work directory.
what they do are as follows:
1. `check_npy.py`
  - check the original COHA npy file that contain vectors to see if there is anomalous
  
2. `check_word_coverage_cleaned.py`
  - to see if we can find meaningful word vectors(not zero or not NaN) for each word in the word lists from all the word vectors.
  - it generates three result files in project root, which are 'vector_quality_summary.csv','word_coverage_report_validated.txt','word_coverage_summary.csv'
  - created by sonnet4.5
  
3. `file_reader.py`
  - read the raw three word vectors

4. `changes_over_time_only_sgns.py`
  - using finalrun.csv to create a finalrun_updated.csv that have overwrited results from only sgns vectors analysis. I create this script so that we don't have to run the whole changes_over_time.py every time we use different sgns vectors. be remember to change the name for finalrun_updated.csv for further analysis.

5. `orgnize_eng_all.py`
  - similar with orgnize_COHA.py
 
## analysis on the dismatch between the plot results.(only for plots in 'used in essay')
1. for heatmap, in every cell there is about 0.1 deviation from the original one. we think it is the result of the standard of round.
2. For Asian occupation bias, the line for Asian occupation difference is the same, but the Asian bias scores before 1950 do not match, and we do not know why. It is worth noting that among the word lists in the data folder, there is a file named 'occupations1950_professional.txt', which corresponds to 1950, and this may mean that there is something wrong with the words in the word list. However, we still do not know for sure.

## Details worth mention
1. The author actually didn't use COHA data but actually used eng-all data of [HistWords](https://nlp.stanford.edu/projects/histwords/)
2. for every two rows in run_params.csv, it actually says which dataset it use and which word lists will apply to this dataset.
3. it takes about 38G RAM to run the `changes_over_time_only_sgns.py` as the cleaned google news vectors is about 19G.
4. In the root path of the replication, there is a backup_files folder to storage imporant files. In "disentangle_finalrun.csv" I tried to display the results in the finalrun.csv to help me figure out why the results of the plots are different.






