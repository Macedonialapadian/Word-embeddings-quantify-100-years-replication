## Overview

This repository contains a replication project for the **Text as Data** course, conducted by **Group: Zhiyang Cheng and Muhammad Saad**.

This project aims at replicating Garg, Nikhil, et al. "Word embeddings quantify 100 years of gender and ethnic stereotypes." Proceedings of the National Academy of Sciences 115.16 (2018): E3635-E3644. which can be find through this [link](https://www.pnas.org/doi/pdf/10.1073/pnas.1720347115)

## Directory Setup

- **Project root:** All scripts should be run from the `code/` directory.

- **Code folder:** `code/`
  This folder contains all Python scripts for analysis and plotting.

- **Data folder:** `data/`
  - `data/word_lists/` - Contains word lists (.txt and .csv files)
  - `data/vectors/raw/` - Raw word vector files (download separately from this [link](https://drive.google.com/drive/folders/1p9-WFXGy-cPFKRNkGl3pi71caAsOhe3x?usp=drive_link))
  - `data/vectors/normalized_clean/` - Processed/normalized vectors
  - `data/vectors/normalized_clean/vocab/` - Vocabulary frequency files

- **Output folder:** `output/`
  - `output/run_results/` - Results generated by `changes_over_time.py` (finalrun.csv)
  - `output/plots/` - Plots generated by `create_final_plots_all.py`
  - `output/regressions/` - Regression results generated by plotting scripts

- **Figure 7:** `output/Figure7/`
  The 'Data' folder under 'Figure 7' is empty, it should be placed with 'newsroom_test.jsonl' files download from this [link](https://drive.google.com/drive/folders/1OpWGtNB-IJTg4nE6U8yRb41mKaPVk-oP?usp=drive_link), and it is originally come from this [project](https://lil.nlp.cornell.edu/newsroom/index.html)

- **Documents:** `documents/`
  Contains research papers and replication documentation

## Replication Steps

All scripts should be run from the `code/` directory.

To replicate the results:

1. Ensure the vectors are placed in `data/vectors/`. Both 'raw' and 'normalized_clean' folders must exist.
  - raw folder contains raw materials, while normalized_clean folder contains cleaned material.
  - changes_over_time.py will only use materials in normalized_clean.

Now we do data preprocessing, the scripts use data in the raw folder to generate organized data in normalized_clean:

2. From the `code/` directory, run `orgnize_COHA.py`.
  - This script creates word counts (in vocab folder) and vectors in normalized_clean from word counts, vocabulary and word vectors in raw folder.
  - The COHA word vectors are themselves normalized, so there is no need to normalize them.

3. Run three normalizer scripts (`normalizer_glove.py`, `normalizer_googlenews.py`, `normalizer_wikipedia.py`).
  - These scripts convert three word vectors to txt and normalize them.

Then we can start to calculate the results and generate the plots:

4. Run `changes_over_time.py`.
  - This creates finalrun.csv in `output/run_results/`. It will add content at the end of the csv, so remove the original finalrun.csv each time you run this script.
  - This script uses run_params.csv, files in normalized_clean, and word lists in `data/word_lists/`.

5. Run `create_final_plots_all.py`.
  - It uses finalrun.csv from `output/run_results/`.
  - This creates plots in `output/plots/` and regression results in `output/regressions/`.
  - This also uses `latexify.py`, `plot_creation.py` and `utilities.py`.

## modification to the author's scripts (except changes from python2 to python3)
1. `changes_over_time.py`
  - comment nyt(new york time data) related in line 271,279,287
  
2. `latexify.py`
  - change the parameter "text.usetex" to False. this is no longer support in the latest version of package, and it was used to create confidential intervals.

3. `utilities.py`
  - there are a few little changes and i think the results will be the same.

4. `plot_creation.py`
  - line 798
  - line 834
  - line 859
  - line 113
  - line 118
  - line 413

5. `create_final_plots_all.py`
  - comment nyt related in line 65
  
there are many little modifications, but I think only the small changes in `utilities.py` and `plot_creation.py` might be concerning if you might want to double check.

6. The replace plot for NYT data
  - In the `code/` folder, run the jupyter notebook `Embeddings_Fig7.ipynb` to get the figure 7 that uses Newsroom_Embeddings as replacement for NYT embeddings.

## use_less_scripts
The `use_less_scripts` folder is under `code/`, which contains auxiliary scripts used for analysis. They should be run from the `code/use_less_scripts/` directory.

What they do are as follows:
1. `check_npy.py`
  - Check the original COHA npy file that contain vectors to see if there is anomalous.

2. `check_word_coverage_cleaned.py`
  - To see if we can find meaningful word vectors (not zero or not NaN) for each word in the word lists from all the word vectors.
  - It generates three result files: 'vector_quality_summary.csv', 'word_coverage_report_validated.txt', 'word_coverage_summary.csv'.
  - Created by sonnet4.5.

3. `file_reader.py`
  - Read the raw three word vectors.

4. `changes_over_time_only_sgns.py`
  - Using finalrun.csv to create a finalrun_updated.csv that has overwritten results from only sgns vectors analysis. This script avoids running the whole changes_over_time.py every time we use different sgns vectors. Remember to change the name for finalrun_updated.csv for further analysis.

5. `orgnize_eng_all.py`
  - Similar to orgnize_COHA.py.
 
## analysis on the dismatch between the plot results.(only for plots in 'used in essay')
1. for heatmap, in every cell there is about 0.1 deviation from the original one. we think it is the result of the standard of round.
2. For Asian occupation bias, the line for Asian occupation difference is the same, but the Asian bias scores before 1950 do not match, and we do not know why. It is worth noting that among the word lists in the data folder, there is a file named 'occupations1950_professional.txt', which corresponds to 1950, and this may mean that there is something wrong with the words in the word list. However, we still do not know for sure.

## Details worth mention
1. The author actually didn't use COHA data but actually used eng-all data of [HistWords](https://nlp.stanford.edu/projects/histwords/)
2. For every two rows in run_params.csv, it actually says which dataset it uses and which word lists will apply to this dataset.
3. It takes about 38G RAM to run `changes_over_time.py` as the cleaned google news vectors is about 19G.
4. There is a `backup_files/` folder to store important files. In "disentangle_finalrun.csv" I tried to display the results in the finalrun.csv to help me figure out why the results of the plots are different.






